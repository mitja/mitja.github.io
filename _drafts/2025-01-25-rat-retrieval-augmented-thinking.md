https://github.com/Doriandarko/RAT-retrieval-augmented-thinking

A tweet by Pietro Schirano (@Doriandarko) on January 24, 2025, introduced a new approach to reasoning called RAT (Retrieval Argument Thinking). The idea is to extract the reasoning process from deepSeek-r1 and send it to any LLM via OpenRouterAI. This process boosts the second LLM's performance and provides access to missing capabilities like function calling and JSON mode.

Retrieval Argument Thinking.  

Extract just the reasoning process from deepSeek-r1 and send it to any LLM via 
@OpenRouterAI
.  
Boost the second LLM's performance and gain access to missing capabilities like function calling and JSON mode.

There is also a specialized implementation designed for Claude models that leverages Anthropic's message prefilling capabilities. This version makes Claude believe the reasoning process is its own internal thought process, leading to more coherent and contextually aware responses.
4:53 PM · Jan 24, 2025
·
5,086
 Views
