{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Langchain Llama.cpp Usage\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/anaconda3/lib/python3.10/site-packages (0.0.334)\n",
      "Requirement already satisfied: langchain-core in /opt/homebrew/anaconda3/lib/python3.10/site-packages (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.62 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (0.5.14)\n",
      "Collecting langchain-core<0.2,>=0.1.21\n",
      "  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-community) (2.0.19)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.21->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.21->langchain-community) (2.4.2)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.21->langchain-community) (1.33)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.21->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.21->langchain-community) (1.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.21->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.21->langchain-community) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.21->langchain-community) (2.10.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Installing collected packages: packaging, langsmith, langchain-core, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.63\n",
      "    Uninstalling langsmith-0.0.63:\n",
      "      Successfully uninstalled langsmith-0.0.63\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit-jupyter 0.2.1 requires ipywidgets==7.7.2, but you have ipywidgets 8.0.4 which is incompatible.\n",
      "poetry 1.6.1 requires jsonschema<4.18.0,>=4.10.0, but you have jsonschema 4.19.2 which is incompatible.\n",
      "myst-parser 0.18.1 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 3.0.0 which is incompatible.\n",
      "myst-parser 0.18.1 requires mdit-py-plugins~=0.3.1, but you have mdit-py-plugins 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 packaging-23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.base import BaseCallbackManager, BaseCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The example in the Langchain documentation didn't work as it was using `langchain.callbacks.manager.CallbackManager` instead of `langchain_core.callbacks.base.BaseCallbackManager`. I found the working example [here](https://medium.com/@jayanthd04/leveraging-llama-to-talk-to-your-codebase-1fc83ed4728c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/mitjamartini/Developer/models/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '18', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system. The path must be an absolute path.\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/mitjamartini/Developer/models/mistral-7b-instruct-v0.1.Q6_K.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INTRO]\n",
      "\n",
      "Stephen Colbert: Ladies and gentlemen, tonight we have a special treat for you. The one and only John Oliver is here to take on the king of late-night comedy, me! That's right, it's time for a rap battle between two of your favorite comedians. So let's get started without further ado.\n",
      "\n",
      "John Oliver: (laughs) Alright Steve, I'm ready to go down swinging. But first, let me just say that I've always been impressed by your ability to turn complex issues into something so entertaining. It's like you're giving us all the news we need but making it fun at the same time. But enough about me, let's get into this rap battle.\n",
      "\n",
      "Stephen Colbert: Yo, yo, yo, listen up, people. We've got two of the funniest guys in town goin' head to head in a rap battle royale. And if you don't know who these guys are, well then you must be living under a rock. But let's not waste any more time on pleasantries, let's get into it!\n",
      "\n",
      "John Oliver: Alright Steve, here's my first line. I may be from England, but I've got the American dream. I'm a comedian who makes fun of politics, it's what I live for, you feel me?\n",
      "\n",
      "Stephen Colbert: (laughs) That was pretty good, John. But let me top that with this one. I'm like a ray of sunshine on a cloudy day, always bringing laughter and light to the people's lives. And if you don't believe me, just ask my fans, they'll vouch for me any day.\n",
      "\n",
      "John Oliver: Well alright Steve, I'll give you that one. But let's see if you can handle this next line. I may be known for my scathing wit, but I've also got some pretty sweet dance moves. You've seen me do the Robot, the Running Man, and even the Moonwalk. What can you do, huh?\n",
      "\n",
      "Stephen Colbert: (smirks) Oh, come on now John. You know I'm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7128.06 ms\n",
      "llama_print_timings:      sample time =      40.53 ms /   496 runs   (    0.08 ms per token, 12238.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7555.79 ms /    16 tokens (  472.24 ms per token,     2.12 tokens per second)\n",
      "llama_print_timings:        eval time =   31352.22 ms /   495 runs   (   63.34 ms per token,    15.79 tokens per second)\n",
      "llama_print_timings:       total time =   39904.06 ms /   511 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[INTRO]\\n\\nStephen Colbert: Ladies and gentlemen, tonight we have a special treat for you. The one and only John Oliver is here to take on the king of late-night comedy, me! That's right, it's time for a rap battle between two of your favorite comedians. So let's get started without further ado.\\n\\nJohn Oliver: (laughs) Alright Steve, I'm ready to go down swinging. But first, let me just say that I've always been impressed by your ability to turn complex issues into something so entertaining. It's like you're giving us all the news we need but making it fun at the same time. But enough about me, let's get into this rap battle.\\n\\nStephen Colbert: Yo, yo, yo, listen up, people. We've got two of the funniest guys in town goin' head to head in a rap battle royale. And if you don't know who these guys are, well then you must be living under a rock. But let's not waste any more time on pleasantries, let's get into it!\\n\\nJohn Oliver: Alright Steve, here's my first line. I may be from England, but I've got the American dream. I'm a comedian who makes fun of politics, it's what I live for, you feel me?\\n\\nStephen Colbert: (laughs) That was pretty good, John. But let me top that with this one. I'm like a ray of sunshine on a cloudy day, always bringing laughter and light to the people's lives. And if you don't believe me, just ask my fans, they'll vouch for me any day.\\n\\nJohn Oliver: Well alright Steve, I'll give you that one. But let's see if you can handle this next line. I may be known for my scathing wit, but I've also got some pretty sweet dance moves. You've seen me do the Robot, the Running Man, and even the Moonwalk. What can you do, huh?\\n\\nStephen Colbert: (smirks) Oh, come on now John. You know I'm\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With an Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://python.langchain.com/docs/modules/model_io/output_parsers/types/json to work with llama.cpp and mistral-7b-instruct-v0.1.Q6_K.gguf\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/mitjamartini/Developer/models/mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '18', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n"
     ]
    }
   ],
   "source": [
    "callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model = LlamaCpp(\n",
    "    model_path=\"/Users/mitjamartini/Developer/models/mistral-7b-instruct-v0.1.Q6_K.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "Tell me a joke.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt #| model | parser\n",
    "\n",
    "from pprint import pprint\n",
    "print(chain.invoke({\"query\": joke_query}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The better approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install py-llm-core --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-17 12:43:47--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
      "Auflösen des Hostnamens huggingface.co (huggingface.co)… 2600:9000:225f:5c00:17:b174:6d00:93a1, 2600:9000:225f:4800:17:b174:6d00:93a1, 2600:9000:225f:9a00:17:b174:6d00:93a1, ...\n",
      "Verbindungsaufbau zu huggingface.co (huggingface.co)|2600:9000:225f:5c00:17:b174:6d00:93a1|:443 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 302 Found\n",
      "Platz: https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1708427738&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQyNzczOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Fb5hxcq1lbVGB%7EV7Vmye29WLxCSxj8JiQFsBx1pD83CeUvNcUUVf%7EebZKAi%7EqehMB1AjdFSUELvMMSn7-WHbg5MpycNfcMi8iqerB9p7XU3OEDbh38aZ21E3WKSeytjGF2v6A8HRvx7rV4nIHkyuAl9nLgXSU2ZN08nXZ5PEDvzR4MEyz%7EEY1cjVJg8oNkXeeLXBbVeScfReuxTRkMLhkAfWv8CucERnkl8yetnIUxKlqiFddz3ZxYCu4wdsZqhg1NobNKdcfeqANxoIGreU92oZyHCFhCCJRtODKzySQvKr1B6qVLBHXBoITsQnG%7EGAqygpG-O7HLATVdPAlKwLyg__&Key-Pair-Id=KVTP0A1DKRTAX [folgend]\n",
      "--2024-02-17 12:43:47--  https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1708427738&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQyNzczOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Fb5hxcq1lbVGB%7EV7Vmye29WLxCSxj8JiQFsBx1pD83CeUvNcUUVf%7EebZKAi%7EqehMB1AjdFSUELvMMSn7-WHbg5MpycNfcMi8iqerB9p7XU3OEDbh38aZ21E3WKSeytjGF2v6A8HRvx7rV4nIHkyuAl9nLgXSU2ZN08nXZ5PEDvzR4MEyz%7EEY1cjVJg8oNkXeeLXBbVeScfReuxTRkMLhkAfWv8CucERnkl8yetnIUxKlqiFddz3ZxYCu4wdsZqhg1NobNKdcfeqANxoIGreU92oZyHCFhCCJRtODKzySQvKr1B6qVLBHXBoITsQnG%7EGAqygpG-O7HLATVdPAlKwLyg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Auflösen des Hostnamens cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)… 2600:9000:225a:3600:11:f807:5180:93a1, 2600:9000:225a:6e00:11:f807:5180:93a1, 2600:9000:225a:6400:11:f807:5180:93a1, ...\n",
      "Verbindungsaufbau zu cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:225a:3600:11:f807:5180:93a1|:443 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 206 Partial Content\n",
      "Länge: 4368438944 (4,1G), 3208600359 (3,0G) sind noch übrig [binary/octet-stream]\n",
      "Wird in »mistral-7b-instruct-v0.1.Q4_K_M.gguf« gespeichert.\n",
      "\n",
      "mistral-7b-instruct  26%[+++++               ]   1,09G  6,74MB/s    in 1,8s    \n",
      "\n",
      "2024-02-17 12:43:49 (6,74 MB/s) - Verbindung bei Byte 1172554675 geschlossen. Erneuter Versuch.\n",
      "\n",
      "--2024-02-17 12:43:50--  (Versuch: 2)  https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1708427738&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODQyNzczOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Fb5hxcq1lbVGB%7EV7Vmye29WLxCSxj8JiQFsBx1pD83CeUvNcUUVf%7EebZKAi%7EqehMB1AjdFSUELvMMSn7-WHbg5MpycNfcMi8iqerB9p7XU3OEDbh38aZ21E3WKSeytjGF2v6A8HRvx7rV4nIHkyuAl9nLgXSU2ZN08nXZ5PEDvzR4MEyz%7EEY1cjVJg8oNkXeeLXBbVeScfReuxTRkMLhkAfWv8CucERnkl8yetnIUxKlqiFddz3ZxYCu4wdsZqhg1NobNKdcfeqANxoIGreU92oZyHCFhCCJRtODKzySQvKr1B6qVLBHXBoITsQnG%7EGAqygpG-O7HLATVdPAlKwLyg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Verbindungsaufbau zu cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:225a:3600:11:f807:5180:93a1|:443 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 206 Partial Content\n",
      "Länge: 4368438944 (4,1G), 3195884269 (3,0G) sind noch übrig [binary/octet-stream]\n",
      "Wird in »mistral-7b-instruct-v0.1.Q4_K_M.gguf« gespeichert.\n",
      "\n",
      "mistral-7b-instruct 100%[+++++==============>]   4,07G  6,69MB/s    in 9m 22s  \n",
      "\n",
      "2024-02-17 12:53:13 (5,42 MB/s) - »mistral-7b-instruct-v0.1.Q4_K_M.gguf« gespeichert [4368438944/4368438944]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For local inference with GGUF models, store your models in MODELS_CACHE_DIR\n",
    "!#mkdir -p ~/.cache/py-llm-core/models\n",
    "!cd ~/.cache/py-llm-core/models && wget -c https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The llama-cpp-python dependency may improperly detects the architecture and raise an error an incompatible architecture (have 'x86_64', need 'arm64')).\n",
    "\n",
    "If that's the case, uncomment and run the following in your virtual env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3.1 from /opt/homebrew/anaconda3/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.44.tar.gz (36.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting scikit-build-core[pyproject]>=0.5.1\n",
      "    Using cached scikit_build_core-0.8.1-py3-none-any.whl (139 kB)\n",
      "  Collecting exceptiongroup\n",
      "    Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "  Collecting packaging>=20.9\n",
      "    Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "  Collecting tomli>=1.1\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Collecting pathspec>=0.10.1\n",
      "    Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "  Collecting pyproject-metadata>=0.5\n",
      "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
      "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "  streamlit-jupyter 0.2.1 requires ipywidgets==7.7.2, but you have ipywidgets 8.0.4 which is incompatible.\n",
      "  poetry 1.6.1 requires jsonschema<4.18.0,>=4.10.0, but you have jsonschema 4.19.2 which is incompatible.\n",
      "  myst-parser 0.18.1 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 3.0.0 which is incompatible.\n",
      "  myst-parser 0.18.1 requires mdit-py-plugins~=0.3.1, but you have mdit-py-plugins 0.4.0 which is incompatible.\n",
      "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.8.1 tomli-2.0.1\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l  Running command pip subprocess to install backend dependencies\n",
      "  Collecting cmake>=3.21\n",
      "    Using cached cmake-3.28.3-py2.py3-none-macosx_10_10_universal2.macosx_10_10_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (48.5 MB)\n",
      "  Installing collected packages: cmake\n",
      "  Successfully installed cmake-3.28.3\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.1\u001b[0m using \u001b[94mCMake 3.28.3\u001b[0m \u001b[91m(metadata_wheel)\u001b[0m\u001b[0m\n",
      "\u001b[?25hdone\n",
      "Collecting jinja2>=2.11.3\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.20.0\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.1\u001b[0m using \u001b[94mCMake 3.28.3\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2024-02-17 13:00:52,707 - scikit_build_core - WARNING - libdir/ldlibrary: /opt/homebrew/anaconda3/lib/libpython3.10.a is not a real file!\n",
      "  2024-02-17 13:00:52,707 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/opt/homebrew/anaconda3/lib, ldlibrary=libpython3.10.a, multiarch=darwin, masd=None\n",
      "  loading initial cache file /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/build/CMakeInit.txt\n",
      "  -- The C compiler identification is AppleClang 15.0.0.15000100\n",
      "  -- The CXX compiler identification is AppleClang 15.0.0.15000100\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-145)\")\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Accelerate framework found\n",
      "  -- Metal framework found\n",
      "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: arm64\n",
      "  -- ARM detected\n",
      "  -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\n",
      "  -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n",
      "  \u001b[33mCMake Warning (dev) at vendor/llama.cpp/CMakeLists.txt:1134 (install):\n",
      "    Target llama has RESOURCE files but no RESOURCE DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:30 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  -- Configuring done (2.6s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/build\n",
      "  \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  Change Dir: '/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/build'\n",
      "\n",
      "  Run Build Command(s): /opt/homebrew/anaconda3/bin/ninja -v\n",
      "  [1/23] cd /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=15.0.0.15000100 -DCMAKE_C_COMPILER_ID=AppleClang -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/Library/Developer/CommandLineTools/usr/bin/cc -P /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
      "  -- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-145)\")\n",
      "  [2/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/build-info.cpp\n",
      "  [3/23] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/ggml-alloc.c\n",
      "  [4/23] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/ggml-backend.c\n",
      "  [5/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/console.cpp\n",
      "  [6/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../.. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../../common -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/llava.cpp\n",
      "  [7/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DGGML_USE_METAL -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../.. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
      "  [8/23] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/ggml-metal.m\n",
      "  [9/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/grammar-parser.cpp\n",
      "  [10/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/sampling.cpp\n",
      "  [11/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/train.cpp\n",
      "  [12/23] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/ggml-quants.c\n",
      "  [13/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/common/common.cpp\n",
      "  [14/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../.. -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/../../common -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/examples/llava/clip.cpp\n",
      "  [15/23] : && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /Library/Developer/CommandLineTools/usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /Library/Developer/CommandLineTools/usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E touch vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
      "  [16/23] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/ggml.c\n",
      "  [17/23] : && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /Library/Developer/CommandLineTools/usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o && /Library/Developer/CommandLineTools/usr/bin/ranlib vendor/llama.cpp/libggml_static.a && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E touch vendor/llama.cpp/libggml_static.a && :\n",
      "  [18/23] : && /Library/Developer/CommandLineTools/usr/bin/cc -O3 -DNDEBUG -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -dynamiclib -Wl,-headerpad_max_install_names  -o vendor/llama.cpp/libggml_shared.dylib -install_name @rpath/libggml_shared.dylib vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o  -framework Accelerate  -framework Foundation  -framework Metal  -framework MetalKit && :\n",
      "  [19/23] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/vendor/llama.cpp/llama.cpp\n",
      "  [20/23] : && /Library/Developer/CommandLineTools/usr/bin/c++ -O3 -DNDEBUG -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -dynamiclib -Wl,-headerpad_max_install_names  -o vendor/llama.cpp/libllama.dylib -install_name @rpath/libllama.dylib vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o  -framework Accelerate  -framework Foundation  -framework Metal  -framework MetalKit && :\n",
      "  [21/23] : && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /Library/Developer/CommandLineTools/usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /Library/Developer/CommandLineTools/usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-build-env-34nkvyzs/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E touch vendor/llama.cpp/common/libcommon.a && :\n",
      "  [22/23] : && /Library/Developer/CommandLineTools/usr/bin/c++ -O3 -DNDEBUG -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -dynamiclib -Wl,-headerpad_max_install_names  -o vendor/llama.cpp/examples/llava/libllava.dylib -install_name @rpath/libllava.dylib vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o  -Wl,-rpath,/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/build/vendor/llama.cpp  vendor/llama.cpp/libllama.dylib  -framework Accelerate  -framework Foundation  -framework Metal  -framework MetalKit && :\n",
      "  [23/23] : && /Library/Developer/CommandLineTools/usr/bin/c++ -O3 -DNDEBUG -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.2.sdk -Wl,-search_paths_first -Wl,-headerpad_max_install_names  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/build/vendor/llama.cpp  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.dylib  -framework Accelerate  -framework Foundation  -framework Metal  -framework MetalKit && :\n",
      "\n",
      "  \u001b[92m***\u001b[0m \u001b[1mInstalling project into wheel...\u001b[0m\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/libggml_shared.dylib\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/include/ggml.h\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/include/ggml-alloc.h\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/include/ggml-backend.h\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/include/ggml-metal.h\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/libllama.dylib\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/include/llama.h\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/convert.py\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/ggml-metal.metal\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/llama_cpp/libllama.dylib\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/llama_cpp/ggml-metal.metal\n",
      "  -- Installing: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/llama_cpp/libllama.dylib\n",
      "  -- Installing: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/llama_cpp/ggml-metal.metal\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/libllava.dylib\n",
      "  [cctools-port]: generating fake signature for '/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/libllava.dylib'\n",
      "  /Library/Developer/CommandLineTools/usr/bin/strip: warning: changes being made to the file will invalidate the code signature in: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/lib/libllava.dylib\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/llava-cli\n",
      "  [cctools-port]: generating fake signature for '/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/llava-cli'\n",
      "  /Library/Developer/CommandLineTools/usr/bin/strip: warning: changes being made to the file will invalidate the code signature in: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/bin/llava-cli\n",
      "  -- Installing: /var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/llama_cpp/libllava.dylib\n",
      "  [cctools-port]: generating fake signature for '/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/llama_cpp/libllava.dylib'\n",
      "  /Library/Developer/CommandLineTools/usr/bin/strip: warning: changes being made to the file will invalidate the code signature in: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/tmpojp82hlf/wheel/platlib/llama_cpp/libllava.dylib\n",
      "  -- Installing: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/llama_cpp/libllava.dylib\n",
      "  [cctools-port]: generating fake signature for '/private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/llama_cpp/libllava.dylib'\n",
      "  /Library/Developer/CommandLineTools/usr/bin/strip: warning: changes being made to the file will invalidate the code signature in: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-install-r19qel3d/llama-cpp-python_7bf2749a4314406c8f84416f2850c3d3/llama_cpp/libllava.dylib\n",
      "  \u001b[92m***\u001b[0m \u001b[1mMaking wheel...\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mCreated\u001b[22m llama_cpp_python-0.2.44-cp310-cp310-macosx_14_0_arm64.whl...\u001b[0m\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.44-cp310-cp310-macosx_14_0_arm64.whl size=2282324 sha256=51d8faef0e1e29dae6753ba9d3972b995683c82a3497b74a4eb900e2a55777bf\n",
      "  Stored in directory: /private/var/folders/7t/gxrc81cj4jv22c_y8l20g9zh0000gn/T/pip-ephem-wheel-cache-esesc9w8/wheels/8e/c0/d1/203d3a954851cda21e101fa7348b8c238440b272d0f481aa76\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/typing_extensions-4.8.0.dist-info/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/bin/f2py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/bin/f2py3\n",
      "      Removing file or directory /opt/homebrew/anaconda3/bin/f2py3.10\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/INSTALLER\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/LICENSE.txt\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/METADATA\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/RECORD\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/REQUESTED\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/WHEEL\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/direct_url.json\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/entry_points.txt\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy-1.25.2.dist-info/top_level.txt\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/LICENSE.txt\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__config__.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__init__.cython-30.pxd\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__init__.pxd\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__init__.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__init__.pyi\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/__pycache__/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_distributor_init.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_globals.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_pyinstaller/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_pytesttester.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_pytesttester.pyi\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_typing/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_utils/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/_version.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/array_api/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/compat/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/conftest.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/core/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/ctypeslib.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/ctypeslib.pyi\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/distutils/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/doc/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/dtypes.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/dtypes.pyi\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/exceptions.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/exceptions.pyi\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/f2py/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/fft/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/lib/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/linalg/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/ma/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/matlib.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/matrixlib/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/polynomial/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/py.typed\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/random/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/setup.py\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/testing/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/tests/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/typing/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/numpy/version.py\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "  changing mode of /opt/homebrew/anaconda3/bin/f2py to 755\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.1\n",
      "    Uninstalling MarkupSafe-2.1.1:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/MarkupSafe-2.1.1.dist-info/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/markupsafe/\n",
      "      Successfully uninstalled MarkupSafe-2.1.1\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/diskcache-5.6.3.dist-info/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/Jinja2-3.1.2.dist-info/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/jinja2/\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.44\n",
      "    Uninstalling llama_cpp_python-0.2.44:\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/bin/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/include/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/lib/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/llama_cpp/\n",
      "      Removing file or directory /opt/homebrew/anaconda3/lib/python3.10/site-packages/llama_cpp_python-0.2.44.dist-info/\n",
      "      Successfully uninstalled llama_cpp_python-0.2.44\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit-jupyter 0.2.1 requires ipywidgets==7.7.2, but you have ipywidgets 8.0.4 which is incompatible.\n",
      "myst-parser 0.18.1 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 3.0.0 which is incompatible.\n",
      "myst-parser 0.18.1 requires mdit-py-plugins~=0.3.1, but you have mdit-py-plugins 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.44 numpy-1.26.4 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DCMAKE_OSX_ARCHITECTURES=arm64\" pip3 install --upgrade --verbose --force-reinstall --no-cache-dir llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example directly from the [py-llm-core](py-llm-core) readme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book(title='Foundation',\n",
      "     summary='Foundation is a science fiction novel by Isaac Asimov. It is a '\n",
      "             'cycle of five interrelated short stories that tell the early '\n",
      "             'story of the Foundation, an institute founded by psychohistorian '\n",
      "             'Hari Seldon to preserve the best of galactic civilization after '\n",
      "             'the collapse of the Galactic Empire.',\n",
      "     author='Isaac Asimov',\n",
      "     published_year=1951)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "from llm_core.parsers import LLaMACPPParser\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "@dataclass\n",
    "class Joke:\n",
    "    setup: str\n",
    "    punchline: str\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "\n",
    "model = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "with LLaMACPPParser(Book, model=model) as parser:\n",
    "    book = parser.parse(text)\n",
    "    pprint(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now adapt this to the joke example. For this we need to split the task into two steps:\n",
    "\n",
    "1. tell me a joke\n",
    "2. parse the output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/mitjamartini/.cache/py-llm-core/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "\n",
      "llama_print_timings:        load time =     465.96 ms\n",
      "llama_print_timings:      sample time =       1.86 ms /    21 runs   (    0.09 ms per token, 11278.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     465.92 ms /    11 tokens (   42.36 ms per token,    23.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1075.95 ms /    20 runs   (   53.80 ms per token,    18.59 tokens per second)\n",
      "llama_print_timings:       total time =    1571.43 ms /    31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10 years ago I said to my wife she was drawing her eyebrows too high she '\n",
      " 'looked surprised.')\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"/Users/mitjamartini/.cache/py-llm-core/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Tell me a joke. A: \", # Prompt\n",
    "      max_tokens=100, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    ")\n",
    "#pprint(output)\n",
    "the_joke = output['choices'][0]['text']\n",
    "pprint(the_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"10 years ago I said to my wife she was drawing her eyebrows too high\" and '\n",
      " 'the punchline is \"She looked surprised.\"')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: Which part is the setup and which the punchline of the following joke? \"10 years ago I said to my wife she was drawing her eyebrows too high. She looked surprised.\" The setup is \n",
    "\"\"\"\n",
    "\n",
    "output = llm(\n",
    "      prompt,\n",
    "      max_tokens=300, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    ")\n",
    "#pprint(output)\n",
    "the_joke_separated = output['choices'][0]['text']\n",
    "pprint(the_joke_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke(setup='10 years ago I said to my wife she was drawing her eyebrows too '\n",
      "           'high',\n",
      "     punchline='She looked surprised.')\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Joke:\n",
    "    setup: str\n",
    "    punchline: str\n",
    "\n",
    "with LLaMACPPParser(Joke, model=model) as parser:\n",
    "    joke_json = parser.parse(the_joke_separated)\n",
    "    pprint(joke_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
